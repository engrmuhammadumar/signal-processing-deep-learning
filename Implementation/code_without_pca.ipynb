{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Directories containing the .mat files\n",
    "directories = {\n",
    "    'inner': r'E:\\Bearings\\Dataset\\Bearing Dataset2\\Inner (1800)',\n",
    "    'outer': r'E:\\Bearings\\Dataset\\Bearing Dataset2\\Outer (1800)',\n",
    "    'roller': r'E:\\Bearings\\Dataset\\Bearing Dataset2\\Roller (1800)',\n",
    "    'normal': r'E:\\Bearings\\Dataset\\Bearing Dataset2\\Normal (1800)'\n",
    "}\n",
    "\n",
    "\n",
    "# Function to load the .mat files\n",
    "def load_mat_files(directory):\n",
    "    data = []\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.mat'):\n",
    "            mat_data = scipy.io.loadmat(os.path.join(directory, file_name))\n",
    "            data.append(mat_data['signal'])  # Replace 'signal' if the key is different\n",
    "    return np.vstack(data)\n",
    "\n",
    "# Load data from each category\n",
    "inner_data = load_mat_files(directories['inner'])\n",
    "outer_data = load_mat_files(directories['outer'])\n",
    "roller_data = load_mat_files(directories['roller'])\n",
    "normal_data = load_mat_files(directories['normal'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import skew, kurtosis\n",
    "import numpy as np\n",
    "\n",
    "# Function to extract features from a signal\n",
    "def extract_features(signal):\n",
    "    features = [\n",
    "        np.mean(signal),\n",
    "        np.min(signal),\n",
    "        np.max(signal),\n",
    "        np.std(signal),\n",
    "        np.var(signal),\n",
    "        np.median(signal),\n",
    "        np.ptp(signal),  # Peak-to-peak range\n",
    "        skew(signal),\n",
    "        kurtosis(signal)\n",
    "    ]\n",
    "    return features\n",
    "\n",
    "# Apply feature extraction to all data\n",
    "def extract_features_from_data(data):\n",
    "    return np.array([extract_features(signal) for signal in data])\n",
    "\n",
    "# Extract features for each category\n",
    "inner_features = extract_features_from_data(inner_data)\n",
    "outer_features = extract_features_from_data(outer_data)\n",
    "roller_features = extract_features_from_data(roller_data)\n",
    "normal_features = extract_features_from_data(normal_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create labels for each category\n",
    "inner_labels = np.full(inner_features.shape[0], 0)  # Label 0 for 'inner'\n",
    "outer_labels = np.full(outer_features.shape[0], 1)  # Label 1 for 'outer'\n",
    "roller_labels = np.full(roller_features.shape[0], 2)  # Label 2 for 'roller'\n",
    "normal_labels = np.full(normal_features.shape[0], 3)  # Label 3 for 'normal'\n",
    "\n",
    "# Combine features and labels\n",
    "X = np.vstack((inner_features, outer_features, roller_features, normal_features))\n",
    "y = np.hstack((inner_labels, outer_labels, roller_labels, normal_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1918, Validation set size: 411, Test set size: 411\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data: 70% training, 15% validation, 15% test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp)\n",
    "\n",
    "# Check the sizes of each set\n",
    "print(f\"Training set size: {len(X_train)}, Validation set size: {len(X_val)}, Test set size: {len(X_test)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the classifiers to evaluate\n",
    "classifiers = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Fine Tree': DecisionTreeClassifier(max_depth=None, min_samples_split=2, random_state=42),\n",
    "    'Medium Tree': DecisionTreeClassifier(max_depth=20, min_samples_split=4, random_state=42),\n",
    "    'Coarse Tree': DecisionTreeClassifier(max_depth=4, min_samples_split=10, random_state=42),\n",
    "    'Linear Discriminant': LinearDiscriminantAnalysis(),\n",
    "    'Quadratic Discriminant': QuadraticDiscriminantAnalysis(),\n",
    "    'Gaussian Naive Bayes': GaussianNB(),\n",
    "    'Linear SVM': SVC(kernel='linear', random_state=42),\n",
    "    'Quadratic SVM': SVC(kernel='poly', degree=2, random_state=42),\n",
    "    'Cubic SVM': SVC(kernel='poly', degree=3, random_state=42),\n",
    "    'Fine Gaussian SVM': SVC(kernel='rbf', gamma=0.75, random_state=42),\n",
    "    'Medium Gaussian SVM': SVC(kernel='rbf', gamma=3, random_state=42),\n",
    "    'Coarse Gaussian SVM': SVC(kernel='rbf', gamma=12, random_state=42),\n",
    "    'Fine KNN': KNeighborsClassifier(n_neighbors=1),\n",
    "    'Medium KNN': KNeighborsClassifier(n_neighbors=10),\n",
    "    'Weighted KNN': KNeighborsClassifier(n_neighbors=10, weights='distance'),\n",
    "    'Ensemble Bagged Tree': BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=30, random_state=42),\n",
    "    'Ensemble Subspace Discriminant': BaggingClassifier(estimator=LinearDiscriminantAnalysis(), n_estimators=30, max_features=0.3, random_state=42),\n",
    "    'Ensemble Subspace KNN': BaggingClassifier(estimator=KNeighborsClassifier(n_neighbors=5), n_estimators=30, max_features=0.5, random_state=42),\n",
    "    'Narrow NN': MLPClassifier(hidden_layer_sizes=(25,), max_iter=1000, random_state=42),\n",
    "    'Medium NN': MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, random_state=42),\n",
    "    'Wide NN': MLPClassifier(hidden_layer_sizes=(25,), max_iter=1000, random_state=42),\n",
    "    'Bilayered NN': MLPClassifier(hidden_layer_sizes=(15, 20), max_iter=1000, random_state=42),\n",
    "    'Trilayered NN': MLPClassifier(hidden_layer_sizes=(10, 10, 10), max_iter=1000, random_state=42)\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined results for both 5 PCA and 9 features saved to combined_classification_results.xlsx with the best test accuracies bolded.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Font\n",
    "\n",
    "# Data for 5 PCA features\n",
    "data_5_pca = {\n",
    "    \"Classifier\": [\n",
    "        \"Random Forest\", \"Fine Tree\", \"Medium Tree\", \"Coarse Tree\", \"Linear Discriminant\", \"Quadratic Discriminant\",\n",
    "        \"Gaussian Naive Bayes\", \"Linear SVM\", \"Quadratic SVM\", \"Cubic SVM\", \"Fine Gaussian SVM\", \"Medium Gaussian SVM\",\n",
    "        \"Coarse Gaussian SVM\", \"Fine KNN\", \"Medium KNN\", \"Weighted KNN\", \"Ensemble Bagged Tree\",\n",
    "        \"Ensemble Subspace Discriminant\", \"Ensemble Subspace KNN\", \"Narrow NN\", \"Medium NN\", \"Wide NN\",\n",
    "        \"Bilayered NN\", \"Trilayered NN\"\n",
    "    ],\n",
    "    \"Validation Accuracy\": [\n",
    "        96.59, 92.68, 92.68, 89.27, 93.17, 95.61, 90.73, 93.66, 27.80, 27.32, 64.88, 51.22, 31.22, 71.22, 57.56, \n",
    "        70.24, 97.56, 82.44, 90.73, 82.44, 73.66, 77.07, 80.49, 76.59\n",
    "    ],\n",
    "    \"Test Accuracy\": [\n",
    "        98.54, 94.66, 94.66, 93.20, 92.72, 97.57, 96.60, 93.20, 30.58, 28.16, 63.11, 50.00, 31.55, 66.50, 51.94, \n",
    "        59.71, 98.54, 83.50, 91.75, 81.55, 73.79, 75.73, 79.13, 77.18\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Data for 9 features\n",
    "data_9_features = {\n",
    "    \"Classifier\": [\n",
    "        \"Random Forest\", \"Fine Tree\", \"Medium Tree\", \"Coarse Tree\", \"Linear Discriminant\", \"Quadratic Discriminant\",\n",
    "        \"Gaussian Naive Bayes\", \"Linear SVM\", \"Quadratic SVM\", \"Cubic SVM\", \"Fine Gaussian SVM\", \"Medium Gaussian SVM\",\n",
    "        \"Coarse Gaussian SVM\", \"Fine KNN\", \"Medium KNN\", \"Weighted KNN\", \"Ensemble Bagged Tree\",\n",
    "        \"Ensemble Subspace Discriminant\", \"Ensemble Subspace KNN\", \"Narrow NN\", \"Medium NN\", \"Wide NN\",\n",
    "        \"Bilayered NN\", \"Trilayered NN\"\n",
    "    ],\n",
    "    \"Validation Accuracy\": [\n",
    "        94.40, 91.00, 90.02, 83.21, 83.45, 88.08, 71.05, 63.75, 57.66, 57.91, 86.86, 88.56, 89.54, 87.35, 88.08, \n",
    "        88.08, 93.43, 66.42, 88.81, 88.32, 88.81, 88.32, 87.83, 88.32\n",
    "    ],\n",
    "    \"Test Accuracy\": [\n",
    "        93.43, 90.27, 91.00, 85.64, 82.97, 87.35, 67.88, 60.58, 52.31, 54.01, 87.10, 88.81, 89.54, 85.40, 89.05, \n",
    "        88.32, 94.89, 62.53, 89.54, 89.78, 91.48, 89.78, 89.54, 88.81\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Creating DataFrames\n",
    "df_5_pca = pd.DataFrame(data_5_pca)\n",
    "df_9_features = pd.DataFrame(data_9_features)\n",
    "\n",
    "# Writing both DataFrames to the same Excel file\n",
    "output_file = \"combined_classification_results.xlsx\"\n",
    "with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "    df_5_pca.to_excel(writer, sheet_name='5 PCA Features', index=False)\n",
    "    df_9_features.to_excel(writer, sheet_name='9 Features', index=False)\n",
    "\n",
    "# Open the workbook to bold the best test accuracies\n",
    "book = load_workbook(output_file)\n",
    "\n",
    "# For 5 PCA features sheet\n",
    "sheet_5_pca = book['5 PCA Features']\n",
    "max_test_accuracy_5_pca = max(data_5_pca[\"Test Accuracy\"])\n",
    "for row in range(2, len(data_5_pca[\"Test Accuracy\"]) + 2):\n",
    "    test_accuracy_cell = sheet_5_pca[f'C{row}']\n",
    "    if test_accuracy_cell.value == max_test_accuracy_5_pca:\n",
    "        test_accuracy_cell.font = Font(bold=True)\n",
    "\n",
    "# For 9 features sheet\n",
    "sheet_9_features = book['9 Features']\n",
    "max_test_accuracy_9_features = max(data_9_features[\"Test Accuracy\"])\n",
    "for row in range(2, len(data_9_features[\"Test Accuracy\"]) + 2):\n",
    "    test_accuracy_cell = sheet_9_features[f'C{row}']\n",
    "    if test_accuracy_cell.value == max_test_accuracy_9_features:\n",
    "        test_accuracy_cell.font = Font(bold=True)\n",
    "\n",
    "# Save the workbook\n",
    "book.save(output_file)\n",
    "\n",
    "print(f\"Combined results for both 5 PCA and 9 features saved to {output_file} with the best test accuracies bolded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
