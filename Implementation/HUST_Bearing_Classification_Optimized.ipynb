{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUST Bearing Fault Diagnosis - Memory-Efficient High Accuracy Classification\n",
    "## Using Smart Augmentation + EfficientNet for 99% Accuracy\n",
    "\n",
    "**Author:** Muhammad Umar  \n",
    "**Dataset:** HUST Bearing (99 samples → ~400 balanced samples)  \n",
    "**Classes:** 4 (Normal, Ball, Inner Race, Outer Race)  \n",
    "**Target:** 99% Accuracy with minimal memory usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat\n",
    "from scipy import signal as scipy_signal\n",
    "import pywt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Memory management\n",
    "import psutil\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}\")\n",
    "print(f\"Available RAM: {psutil.virtual_memory().available / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = r'F:\\NeuTech\\HUST bearing\\HUST bearing dataset'\n",
    "OUTPUT_PATH = r'F:\\NeuTech\\Results\\HUST_Classification'\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Signal parameters - OPTIMIZED FOR MEMORY\n",
    "SAMPLING_FREQ = 51200\n",
    "SEGMENT_LENGTH = 2048  # Segment size\n",
    "SAMPLES_PER_FILE = 4   # Only 4 segments per file (controlled augmentation)\n",
    "\n",
    "# CWT parameters - REDUCED SIZE\n",
    "SCALES = np.arange(1, 64)  # Reduced from 128 to 64\n",
    "WAVELET = 'morl'\n",
    "IMAGE_SIZE = (128, 128)  # Reduced from 224 to 128 for memory\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 16  # Reduced batch size\n",
    "EPOCHS = 100\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Class mapping\n",
    "CLASS_NAMES = ['Normal', 'Ball', 'Inner', 'Outer']\n",
    "\n",
    "print(\"✓ Configuration set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Organize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hust_data(data_path):\n",
    "    \"\"\"Load and organize HUST dataset by fault type\"\"\"\n",
    "    print(\"Loading HUST bearing dataset...\\n\")\n",
    "    \n",
    "    all_files = [f for f in os.listdir(data_path) if f.endswith('.mat')]\n",
    "    \n",
    "    data_dict = {'Normal': [], 'Ball': [], 'Inner': [], 'Outer': []}\n",
    "    \n",
    "    for file in all_files:\n",
    "        file_upper = file.upper()\n",
    "        if file_upper.startswith('N'):\n",
    "            data_dict['Normal'].append(os.path.join(data_path, file))\n",
    "        elif file_upper.startswith('B') and not file_upper.startswith('BA'):\n",
    "            data_dict['Ball'].append(os.path.join(data_path, file))\n",
    "        elif file_upper.startswith('I'):\n",
    "            data_dict['Inner'].append(os.path.join(data_path, file))\n",
    "        elif file_upper.startswith('O'):\n",
    "            data_dict['Outer'].append(os.path.join(data_path, file))\n",
    "    \n",
    "    print(\"Dataset Statistics:\")\n",
    "    print(\"=\"*50)\n",
    "    for name, files in data_dict.items():\n",
    "        print(f\"{name:15s}: {len(files):3d} files\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "data_dict = load_hust_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Memory-Efficient Signal Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signal_from_mat(filepath):\n",
    "    \"\"\"Load vibration signal from .mat file\"\"\"\n",
    "    try:\n",
    "        data = loadmat(filepath)\n",
    "        for key in data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                signal_data = data[key]\n",
    "                if isinstance(signal_data, np.ndarray) and signal_data.size > 100:\n",
    "                    return signal_data.flatten()\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def extract_segments(signal, segment_length, num_segments=4):\n",
    "    \"\"\"Extract fixed number of random segments from signal\"\"\"\n",
    "    segments = []\n",
    "    signal_length = len(signal)\n",
    "    \n",
    "    if signal_length < segment_length:\n",
    "        return []\n",
    "    \n",
    "    # Random starting positions\n",
    "    max_start = signal_length - segment_length\n",
    "    starts = np.random.choice(max_start, size=min(num_segments, max_start), replace=False)\n",
    "    \n",
    "    for start in starts:\n",
    "        segment = signal[start:start + segment_length]\n",
    "        segments.append(segment)\n",
    "    \n",
    "    return segments\n",
    "\n",
    "def signal_to_cwt_image(signal, scales, wavelet='morl', target_size=(128, 128)):\n",
    "    \"\"\"Convert signal to CWT image - memory efficient\"\"\"\n",
    "    # Normalize\n",
    "    signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-8)\n",
    "    \n",
    "    # Compute CWT - use float32 to save memory\n",
    "    coefficients, _ = pywt.cwt(signal.astype(np.float32), scales, wavelet)\n",
    "    \n",
    "    # Get magnitude\n",
    "    cwt_abs = np.abs(coefficients)\n",
    "    \n",
    "    # Normalize to 0-1\n",
    "    cwt_norm = (cwt_abs - cwt_abs.min()) / (cwt_abs.max() - cwt_abs.min() + 1e-8)\n",
    "    \n",
    "    # Resize using simple method (faster, less memory)\n",
    "    from skimage.transform import resize\n",
    "    cwt_resized = resize(cwt_norm, target_size, mode='reflect', anti_aliasing=True)\n",
    "    \n",
    "    # Convert to RGB\n",
    "    cwt_rgb = np.stack([cwt_resized] * 3, axis=-1)\n",
    "    \n",
    "    return (cwt_rgb * 255).astype(np.uint8)\n",
    "\n",
    "print(\"✓ Signal processing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Balanced Dataset with Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_balanced_dataset(data_dict, samples_per_file=4):\n",
    "    \"\"\"Generate balanced dataset with memory management\"\"\"\n",
    "    X_images = []\n",
    "    y_labels = []\n",
    "    \n",
    "    print(\"\\nGenerating CWT images (balanced sampling)...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for class_idx, (class_name, file_paths) in enumerate(data_dict.items()):\n",
    "        print(f\"\\nProcessing {class_name}...\")\n",
    "        class_count = 0\n",
    "        \n",
    "        for file_idx, file_path in enumerate(file_paths):\n",
    "            # Load signal\n",
    "            signal_data = load_signal_from_mat(file_path)\n",
    "            if signal_data is None:\n",
    "                continue\n",
    "            \n",
    "            # Extract fixed number of segments\n",
    "            segments = extract_segments(signal_data, SEGMENT_LENGTH, samples_per_file)\n",
    "            \n",
    "            for segment in segments:\n",
    "                # Generate CWT image\n",
    "                cwt_img = signal_to_cwt_image(segment, SCALES, WAVELET, IMAGE_SIZE)\n",
    "                \n",
    "                X_images.append(cwt_img)\n",
    "                y_labels.append(class_idx)\n",
    "                class_count += 1\n",
    "            \n",
    "            # Clear memory periodically\n",
    "            if (file_idx + 1) % 10 == 0:\n",
    "                gc.collect()\n",
    "        \n",
    "        print(f\"  Generated {class_count} images\")\n",
    "    \n",
    "    X_images = np.array(X_images, dtype=np.uint8)\n",
    "    y_labels = np.array(y_labels, dtype=np.int32)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Total images: {len(X_images)}\")\n",
    "    print(f\"Image shape: {X_images[0].shape}\")\n",
    "    print(f\"Memory usage: {X_images.nbytes / (1024**2):.2f} MB\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return X_images, y_labels\n",
    "\n",
    "# Generate dataset\n",
    "X_data, y_data = generate_balanced_dataset(data_dict, SAMPLES_PER_FILE)\n",
    "\n",
    "# Show distribution\n",
    "unique, counts = np.unique(y_data, return_counts=True)\n",
    "print(\"Dataset Distribution:\")\n",
    "for i, count in zip(unique, counts):\n",
    "    print(f\"  {CLASS_NAMES[i]:15s}: {count:4d} samples ({count/len(y_data)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(4):\n",
    "    class_indices = np.where(y_data == i)[0]\n",
    "    samples = np.random.choice(class_indices, 2, replace=False)\n",
    "    \n",
    "    for j, idx in enumerate(samples):\n",
    "        axes[i*2 + j].imshow(X_data[idx], cmap='jet')\n",
    "        axes[i*2 + j].set_title(f'{CLASS_NAMES[i]} - Sample {j+1}', \n",
    "                               fontweight='bold', fontsize=12)\n",
    "        axes[i*2 + j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'sample_cwt_images.png'), dpi=200, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42, stratify=y_data\n",
    ")\n",
    "\n",
    "# Normalize\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# One-hot encode\n",
    "y_train_cat = keras.utils.to_categorical(y_train, 4)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, 4)\n",
    "\n",
    "print(\"Train/Test Split:\")\n",
    "print(f\"  Train: {len(X_train)} samples\")\n",
    "print(f\"  Test:  {len(X_test)} samples\")\n",
    "\n",
    "# Clear original data from memory\n",
    "del X_data, y_data\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n✓ Data ready for training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build Optimized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_efficientnet_model(input_shape=(128, 128, 3), num_classes=4):\n",
    "    \"\"\"Build EfficientNet model optimized for small input\"\"\"\n",
    "    \n",
    "    base_model = EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Freeze base model\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(256, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = build_efficientnet_model()\n",
    "print(\"\\n✓ Model built!\")\n",
    "print(f\"Total parameters: {model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Augmentation During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Data augmentation generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=False,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "print(\"✓ Data augmentation configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=15, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7, verbose=1),\n",
    "    ModelCheckpoint(os.path.join(OUTPUT_PATH, 'best_model.h5'), \n",
    "                   monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting Training...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train_cat, batch_size=BATCH_SIZE),\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=(X_test, y_test_cat),\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax1.plot(history.history['accuracy'], 'b-', label='Train', linewidth=2)\n",
    "ax1.plot(history.history['val_accuracy'], 'r-', label='Validation', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontweight='bold', fontsize=12)\n",
    "ax1.set_ylabel('Accuracy', fontweight='bold', fontsize=12)\n",
    "ax1.set_title('Model Accuracy', fontweight='bold', fontsize=14)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(history.history['loss'], 'b-', label='Train', linewidth=2)\n",
    "ax2.plot(history.history['val_loss'], 'r-', label='Validation', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontweight='bold', fontsize=12)\n",
    "ax2.set_ylabel('Loss', fontweight='bold', fontsize=12)\n",
    "ax2.set_title('Model Loss', fontweight='bold', fontsize=14)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'training_history.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "best_acc = max(history.history['val_accuracy'])\n",
    "print(f\"\\nBest Validation Accuracy: {best_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluate and Generate Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = keras.models.load_model(os.path.join(OUTPUT_PATH, 'best_model.h5'))\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "# Predictions\n",
    "y_pred_proba = model.predict(X_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "            annot_kws={'size': 16, 'weight': 'bold'})\n",
    "plt.xlabel('Predicted', fontweight='bold', fontsize=14)\n",
    "plt.ylabel('True', fontweight='bold', fontsize=14)\n",
    "plt.title(f'Confusion Matrix - Test Accuracy: {test_acc*100:.2f}%',\n",
    "         fontweight='bold', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\" + classification_report(y_test, y_pred, target_names=CLASS_NAMES, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model.save(os.path.join(OUTPUT_PATH, 'hust_model_final.h5'))\n",
    "\n",
    "# Save summary\n",
    "summary = {\n",
    "    'Dataset': 'HUST Bearing',\n",
    "    'Total Samples': len(X_train) + len(X_test),\n",
    "    'Train': len(X_train),\n",
    "    'Test': len(X_test),\n",
    "    'Test Accuracy': f\"{test_acc*100:.2f}%\",\n",
    "    'Image Size': IMAGE_SIZE,\n",
    "    'Model': 'EfficientNetB0'\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(os.path.join(OUTPUT_PATH, 'results.json'), 'w') as f:\n",
    "    json.dump(summary, f, indent=4)\n",
    "\n",
    "print(\"\\n✓ All results saved to:\", OUTPUT_PATH)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for k, v in summary.items():\n",
    "    print(f\"{k:20s}: {v}\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
