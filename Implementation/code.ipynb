{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfdeb68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os, math, random, json, time\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ---- reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---- your dataset layout\n",
    "SENSOR_DIRS = {\n",
    "    \"Accelerometer\":  r\"E:\\Upwork Project\\AI_Leak_Detection_Project\\images\\cwt_log\\Accelerometer\\Looped\",\n",
    "    \"DynamicPressure\":r\"E:\\Upwork Project\\AI_Leak_Detection_Project\\images\\cwt_log\\Dynamic Pressure Sensor\\Looped\",\n",
    "    \"Hydrophone\":     r\"E:\\Upwork Project\\AI_Leak_Detection_Project\\images\\cwt_log\\Hydrophones\\Looped\",\n",
    "}\n",
    "CLASSES = [\n",
    "    \"No-leak\",\n",
    "    \"Orifice Leak\",\n",
    "    \"Gasket Leak\",\n",
    "    \"Longitudinal Crack\",\n",
    "    \"Circumferential Crack\",\n",
    "]\n",
    "CLASS2IDX = {c:i for i,c in enumerate(CLASSES)}\n",
    "\n",
    "# ---- image + sequence sizing\n",
    "IMG_H, IMG_W = 128, 256     # resize HxW for CWT (freq x time)\n",
    "T_STEPS       = 16          # split time axis into 16 slices\n",
    "SLICE_W       = IMG_W // T_STEPS  # 16px per step\n",
    "\n",
    "# ---- training hyperparams\n",
    "BATCH_SIZE    = 16\n",
    "EPOCHS        = 40\n",
    "LR            = 3e-4\n",
    "WEIGHT_DECAY  = 1e-4\n",
    "DROPOUT       = 0.2\n",
    "HID_CNN       = 64          # base channels in CNN\n",
    "LSTM_HID      = 256\n",
    "LABEL_SMOOTH  = 0.05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b2e985f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARN] Poor filename overlap for class 'No-leak'. Falling back to sorted alignment.\n",
      "[WARN] Poor filename overlap for class 'Orifice Leak'. Falling back to sorted alignment.\n",
      "[WARN] Poor filename overlap for class 'Gasket Leak'. Falling back to sorted alignment.\n",
      "[WARN] Poor filename overlap for class 'Longitudinal Crack'. Falling back to sorted alignment.\n",
      "[WARN] Poor filename overlap for class 'Circumferential Crack'. Falling back to sorted alignment.\n",
      "Total triplets: 1250 (expected ~ 3750)\n",
      "Splits -> train:874, val:188, test:188\n"
     ]
    }
   ],
   "source": [
    "IMG_EXTS = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"}\n",
    "\n",
    "def list_files(d: Path) -> Dict[str, List[Path]]:\n",
    "    d = Path(d)\n",
    "    out = {}\n",
    "    for c in CLASSES:\n",
    "        class_dir = d / c\n",
    "        if not class_dir.exists():\n",
    "            raise FileNotFoundError(f\"Missing class folder: {class_dir}\")\n",
    "        files = [p for p in class_dir.rglob(\"*\") if p.suffix.lower() in IMG_EXTS]\n",
    "        out[c] = files\n",
    "    return out\n",
    "\n",
    "sensor_files = {s: list_files(Path(p)) for s,p in SENSOR_DIRS.items()}\n",
    "\n",
    "def stemset(paths: List[Path]) -> Dict[str, Path]:\n",
    "    # normalize stems (without extension)\n",
    "    return {p.stem.lower(): p for p in paths}\n",
    "\n",
    "def build_triplets() -> List[Tuple[Path,Path,Path,int]]:\n",
    "    triplets = []\n",
    "    for c in CLASSES:\n",
    "        acc = sensor_files[\"Accelerometer\"][c]\n",
    "        dyn = sensor_files[\"DynamicPressure\"][c]\n",
    "        hyd = sensor_files[\"Hydrophone\"][c]\n",
    "\n",
    "        A, D, H = stemset(acc), stemset(dyn), stemset(hyd)\n",
    "        common = set(A.keys()) & set(D.keys()) & set(H.keys())\n",
    "\n",
    "        # If naming differs, fallback to positional alignment\n",
    "        if len(common) < 0.8*min(len(acc), len(dyn), len(hyd)):\n",
    "            print(f\"[WARN] Poor filename overlap for class '{c}'. Falling back to sorted alignment.\")\n",
    "            tripN = min(len(acc), len(dyn), len(hyd))\n",
    "            acc_s = sorted(acc)[:tripN]\n",
    "            dyn_s = sorted(dyn)[:tripN]\n",
    "            hyd_s = sorted(hyd)[:tripN]\n",
    "            for i in range(tripN):\n",
    "                triplets.append((acc_s[i], dyn_s[i], hyd_s[i], CLASS2IDX[c]))\n",
    "        else:\n",
    "            for k in sorted(common):\n",
    "                triplets.append((A[k], D[k], H[k], CLASS2IDX[c]))\n",
    "\n",
    "    print(f\"Total triplets: {len(triplets)} (expected ~ 3750)\")\n",
    "    return triplets\n",
    "\n",
    "TRIPLETS = build_triplets()\n",
    "\n",
    "# split (stratified by class id)\n",
    "y = [t[3] for t in TRIPLETS]\n",
    "train_ids, test_ids = train_test_split(np.arange(len(TRIPLETS)), test_size=0.15, stratify=y, random_state=42)\n",
    "y_train = [TRIPLETS[i][3] for i in train_ids]\n",
    "train_ids, val_ids  = train_test_split(train_ids, test_size=0.1765, stratify=y_train, random_state=42)  # 0.1765 of 85% â‰ˆ 15% total\n",
    "print(f\"Splits -> train:{len(train_ids)}, val:{len(val_ids)}, test:{len(test_ids)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc47cec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToNumpyGray:\n",
    "    def __call__(self, img: Image.Image):\n",
    "        if img.mode != \"L\":\n",
    "            img = img.convert(\"L\")\n",
    "        return np.array(img, dtype=np.float32)\n",
    "\n",
    "class ZScorePerImage:\n",
    "    def __call__(self, x: np.ndarray):\n",
    "        m, s = float(x.mean()), float(x.std() + 1e-6)\n",
    "        return (x - m) / s\n",
    "\n",
    "class TimeFreqMask:\n",
    "    \"\"\"Random time/freq masking (SpecAugment-like) on the 2D map.\"\"\"\n",
    "    def __init__(self, time_mask_frac=0.1, freq_mask_frac=0.1, p=0.7):\n",
    "        self.tfrac = time_mask_frac\n",
    "        self.ffrac = freq_mask_frac\n",
    "        self.p = p\n",
    "    def __call__(self, x: np.ndarray):\n",
    "        if random.random() < self.p:\n",
    "            H, W = x.shape\n",
    "            # time mask\n",
    "            t = int(W * self.tfrac)\n",
    "            if t > 0:\n",
    "                t0 = random.randint(0, max(0, W - t))\n",
    "                x[:, t0:t0+t] = 0\n",
    "            # freq mask\n",
    "            f = int(H * self.ffrac)\n",
    "            if f > 0:\n",
    "                f0 = random.randint(0, max(0, H - f))\n",
    "                x[f0:f0+f, :] = 0\n",
    "        return x\n",
    "\n",
    "def make_transform(train=True):\n",
    "    aug = []\n",
    "    aug.append(ToNumpyGray())\n",
    "    if train:\n",
    "        aug.append(TimeFreqMask(0.1, 0.1, p=0.7))\n",
    "    aug.append(ZScorePerImage())\n",
    "    def _tf(img: Image.Image):\n",
    "        x = img\n",
    "        for a in aug:\n",
    "            x = a(x)\n",
    "        # resize to (IMG_H, IMG_W)\n",
    "        x = Image.fromarray(((x - x.min()) / (x.max()-x.min()+1e-8) * 255.0).astype(np.uint8))\n",
    "        x = x.resize((IMG_W, IMG_H), resample=Image.BILINEAR)\n",
    "        x = np.array(x, dtype=np.float32)\n",
    "        return x  # (H, W)\n",
    "    return _tf\n",
    "\n",
    "train_tf = make_transform(train=True)\n",
    "eval_tf  = make_transform(train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1647b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Upwork Project\\AI Leak\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "class CWTSequenceDataset(Dataset):\n",
    "    def __init__(self, triplets, indices, transform):\n",
    "        self.triplets = triplets\n",
    "        self.indices = list(indices)\n",
    "        self.tf = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "    def _load_gray(self, p: Path):\n",
    "        img = Image.open(p)\n",
    "        return self.tf(img)  # (H,W) float32\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        idx = self.indices[i]\n",
    "        p_acc, p_dyn, p_hyd, y = self.triplets[idx]\n",
    "\n",
    "        a = self._load_gray(p_acc)\n",
    "        d = self._load_gray(p_dyn)\n",
    "        h = self._load_gray(p_hyd)\n",
    "\n",
    "        # stack sensors as channels\n",
    "        x = np.stack([a, d, h], axis=0)  # (3, H, W)\n",
    "\n",
    "        # split along time into T_STEPS slices\n",
    "        slices = []\n",
    "        for t in range(T_STEPS):\n",
    "            t0, t1 = t*SLICE_W, (t+1)*SLICE_W\n",
    "            patch = x[:, :, t0:t1]  # (3, H, slice_w)\n",
    "            slices.append(patch)\n",
    "        seq = np.stack(slices, axis=0)  # (T, 3, H, slice_w)\n",
    "\n",
    "        return torch.from_numpy(seq), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "train_ds = CWTSequenceDataset(TRIPLETS, train_ids, train_tf)\n",
    "val_ds   = CWTSequenceDataset(TRIPLETS, val_ids,   eval_tf)\n",
    "test_ds  = CWTSequenceDataset(TRIPLETS, test_ids,  eval_tf)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=4, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Batch shapes:\", xb.shape, yb.shape)  # (B, T, C, H, W) ; (B,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b618c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out, k=3, s=1, p=1, pool=True, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(c_in, c_out, k, s, p, bias=False),\n",
    "            nn.BatchNorm2d(c_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(c_out, c_out, k, 1, p, bias=False),\n",
    "            nn.BatchNorm2d(c_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(2) if pool else nn.Identity()\n",
    "        self.drop = nn.Dropout2d(dropout)\n",
    "        # Simple Squeeze-Excite\n",
    "        self.se_fc1 = nn.Linear(c_out, c_out//4)\n",
    "        self.se_fc2 = nn.Linear(c_out//4, c_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        # SE\n",
    "        b,c,h,w = x.shape\n",
    "        s = x.mean(dim=(2,3))                    # (b,c)\n",
    "        s = F.relu(self.se_fc1(s))\n",
    "        s = torch.sigmoid(self.se_fc2(s)).view(b,c,1,1)\n",
    "        x = x * s\n",
    "        x = self.pool(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, c_in=3, base=HID_CNN, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.seq = nn.Sequential(\n",
    "            ConvBlock(c_in,   base,   pool=True,  dropout=dropout/2),\n",
    "            ConvBlock(base,   base*2, pool=True,  dropout=dropout/2),\n",
    "            ConvBlock(base*2, base*4, pool=True,  dropout=dropout),\n",
    "        )\n",
    "        self.out_channels = base*4\n",
    "\n",
    "    def forward(self, x):   # x: (B, C, H, W)\n",
    "        x = self.seq(x)     # (B, C', H', W')\n",
    "        x = x.mean(dim=(2,3))  # global avg pool â†’ (B, C')\n",
    "        return x\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, n_classes=len(CLASSES), c_in=3, base=HID_CNN, lstm_h=LSTM_HID, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.encoder = CNNEncoder(c_in=c_in, base=base, dropout=dropout)\n",
    "        self.lstm = nn.LSTM(input_size=self.encoder.out_channels,\n",
    "                            hidden_size=lstm_h,\n",
    "                            num_layers=2,\n",
    "                            batch_first=True,\n",
    "                            dropout=dropout,\n",
    "                            bidirectional=True)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(lstm_h*2),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_h*2, n_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # x: (B, T, C, H, W)\n",
    "        B, T, C, H, W = x.shape\n",
    "        x = x.view(B*T, C, H, W)\n",
    "        feats = self.encoder(x)          # (B*T, F)\n",
    "        feats = feats.view(B, T, -1)     # (B, T, F)\n",
    "        out, _ = self.lstm(feats)        # (B, T, 2*H)\n",
    "        # use last time step (or pooling over time)\n",
    "        out = out[:, -1, :]              # (B, 2*H)\n",
    "        logits = self.head(out)          # (B, n_classes)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b915b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCE(nn.Module):\n",
    "    def __init__(self, eps=0.0):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, logits, target):\n",
    "        n = logits.size(-1)\n",
    "        logp = F.log_softmax(logits, dim=-1)\n",
    "        with torch.no_grad():\n",
    "            true = torch.zeros_like(logp).fill_(self.eps / (n - 1))\n",
    "            true.scatter_(1, target.unsqueeze(1), 1 - self.eps)\n",
    "        return torch.mean(torch.sum(-true * logp, dim=-1))\n",
    "\n",
    "def accuracy(logits, y):\n",
    "    return (logits.argmax(dim=1) == y).float().mean().item()\n",
    "\n",
    "model = CNN_LSTM().to(device)\n",
    "criterion = LabelSmoothingCE(eps=LABEL_SMOOTH)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(device.type==\"cuda\"))\n",
    "\n",
    "def run_epoch(loader, train=True):\n",
    "    model.train(train)\n",
    "    total_loss, total_acc, n = 0.0, 0.0, 0\n",
    "    for xb, yb in tqdm(loader, leave=False):\n",
    "        xb, yb = xb.to(device, non_blocking=True), yb.to(device, non_blocking=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(device.type==\"cuda\")), torch.set_grad_enabled(train):\n",
    "            logits = model(xb)\n",
    "            loss = criterion(logits, yb)\n",
    "        if train:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "        total_acc  += (logits.argmax(1) == yb).float().sum().item()\n",
    "        n += xb.size(0)\n",
    "    if (not train):\n",
    "        return total_loss / n, total_acc / n\n",
    "    else:\n",
    "        scheduler.step()\n",
    "        return total_loss / n, total_acc / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a2fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0.0\n",
    "patience, bad_epochs = 8, 0\n",
    "ckpt_path = \"cnn_lstm_best.pt\"\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, train=True)\n",
    "    vl_loss, vl_acc = run_epoch(val_loader,   train=False)\n",
    "    if vl_acc > best_val_acc:\n",
    "        best_val_acc = vl_acc\n",
    "        bad_epochs = 0\n",
    "        torch.save({\"model\": model.state_dict(),\n",
    "                    \"cfg\": {\"IMG_H\":IMG_H, \"IMG_W\":IMG_W, \"T_STEPS\":T_STEPS, \"SLICE_W\":SLICE_W,\n",
    "                            \"CLASSES\":CLASSES}}, ckpt_path)\n",
    "    else:\n",
    "        bad_epochs += 1\n",
    "\n",
    "    print(f\"Epoch {epoch:02d}/{EPOCHS} | \"\n",
    "          f\"train loss {tr_loss:.4f} acc {tr_acc*100:.2f}% | \"\n",
    "          f\"val loss {vl_loss:.4f} acc {vl_acc*100:.2f}% | \"\n",
    "          f\"best val {best_val_acc*100:.2f}%\")\n",
    "\n",
    "    if bad_epochs >= patience:\n",
    "        print(\"Early stopping.\")\n",
    "        break\n",
    "\n",
    "print(\"Best validation accuracy:\", best_val_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0ab8e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
