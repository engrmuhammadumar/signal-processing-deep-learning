{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HUST Bearing Fault Diagnosis - High Accuracy Classification\n",
    "## Using Data Augmentation + EfficientNet for 99% Accuracy\n",
    "\n",
    "**Author:** Muhammad Umar  \n",
    "**Dataset:** HUST Bearing (99 samples → augmented to 1000+ samples)  \n",
    "**Classes:** 4 (Normal, Ball, Inner Race, Outer Race)  \n",
    "**Target:** 99% Accuracy with minimal training time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.io import loadmat\n",
    "from scipy import signal\n",
    "import pywt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU Available: {tf.config.list_physical_devices('GPU')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths\n",
    "DATA_PATH = r'F:\\NeuTech\\HUST bearing\\HUST bearing dataset'\n",
    "OUTPUT_PATH = r'F:\\NeuTech\\Results\\HUST_Classification'\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Signal parameters\n",
    "SAMPLING_FREQ = 51200  # Hz\n",
    "SEGMENT_LENGTH = 2048  # Samples per segment\n",
    "OVERLAP = 0.5  # 50% overlap for augmentation\n",
    "\n",
    "# CWT parameters for image generation\n",
    "SCALES = np.arange(1, 128)\n",
    "WAVELET = 'morl'\n",
    "IMAGE_SIZE = (224, 224)  # EfficientNet input size\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Class mapping\n",
    "CLASS_NAMES = ['Normal', 'Ball', 'Inner', 'Outer']\n",
    "\n",
    "print(\"Configuration set!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hust_data(data_path):\n",
    "    \"\"\"\n",
    "    Load HUST bearing dataset and organize by fault type\n",
    "    \"\"\"\n",
    "    print(\"Loading HUST bearing dataset...\")\n",
    "    \n",
    "    # Get all .mat files\n",
    "    all_files = [f for f in os.listdir(data_path) if f.endswith('.mat')]\n",
    "    \n",
    "    # Organize by class\n",
    "    data_dict = {\n",
    "        'Normal': [],\n",
    "        'Ball': [],\n",
    "        'Inner': [],\n",
    "        'Outer': []\n",
    "    }\n",
    "    \n",
    "    for file in all_files:\n",
    "        file_upper = file.upper()\n",
    "        \n",
    "        if file_upper.startswith('N'):\n",
    "            data_dict['Normal'].append(os.path.join(data_path, file))\n",
    "        elif file_upper.startswith('B') and not file_upper.startswith('BA'):\n",
    "            data_dict['Ball'].append(os.path.join(data_path, file))\n",
    "        elif file_upper.startswith('I'):\n",
    "            data_dict['Inner'].append(os.path.join(data_path, file))\n",
    "        elif file_upper.startswith('O'):\n",
    "            data_dict['Outer'].append(os.path.join(data_path, file))\n",
    "    \n",
    "    # Print statistics\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"Dataset Statistics:\")\n",
    "    print(\"=\"*50)\n",
    "    total = 0\n",
    "    for class_name, files in data_dict.items():\n",
    "        count = len(files)\n",
    "        total += count\n",
    "        print(f\"{class_name:15s}: {count:3d} files\")\n",
    "    print(f\"{'Total':15s}: {total:3d} files\")\n",
    "    print(\"=\"*50 + \"\\n\")\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "# Load data\n",
    "data_dict = load_hust_data(DATA_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Signal Processing and Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_signal_from_mat(filepath):\n",
    "    \"\"\"\n",
    "    Load vibration signal from HUST .mat file\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data = loadmat(filepath)\n",
    "        \n",
    "        # Try common field names\n",
    "        for key in data.keys():\n",
    "            if not key.startswith('__'):\n",
    "                signal_data = data[key]\n",
    "                if isinstance(signal_data, np.ndarray) and signal_data.size > 100:\n",
    "                    return signal_data.flatten()\n",
    "        return None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def segment_signal(signal, segment_length, overlap=0.5):\n",
    "    \"\"\"\n",
    "    Segment signal with overlap for data augmentation\n",
    "    \"\"\"\n",
    "    step = int(segment_length * (1 - overlap))\n",
    "    segments = []\n",
    "    \n",
    "    for start in range(0, len(signal) - segment_length + 1, step):\n",
    "        segment = signal[start:start + segment_length]\n",
    "        segments.append(segment)\n",
    "    \n",
    "    return segments\n",
    "\n",
    "def augment_signal(signal, num_augmentations=3):\n",
    "    \"\"\"\n",
    "    Apply data augmentation techniques to vibration signals\n",
    "    \"\"\"\n",
    "    augmented = [signal]  # Original signal\n",
    "    \n",
    "    # 1. Add noise\n",
    "    noise_level = 0.01 * np.std(signal)\n",
    "    noisy = signal + np.random.normal(0, noise_level, signal.shape)\n",
    "    augmented.append(noisy)\n",
    "    \n",
    "    # 2. Scale amplitude\n",
    "    scale_factors = [0.9, 1.1]\n",
    "    for scale in scale_factors[:num_augmentations-1]:\n",
    "        scaled = signal * scale\n",
    "        augmented.append(scaled)\n",
    "    \n",
    "    # 3. Time shift\n",
    "    shift = int(len(signal) * 0.1)\n",
    "    shifted = np.roll(signal, shift)\n",
    "    augmented.append(shifted)\n",
    "    \n",
    "    return augmented[:num_augmentations + 1]\n",
    "\n",
    "def signal_to_cwt_image(signal, scales, wavelet='morl'):\n",
    "    \"\"\"\n",
    "    Convert 1D signal to 2D CWT image\n",
    "    \"\"\"\n",
    "    # Normalize signal\n",
    "    signal = (signal - np.mean(signal)) / (np.std(signal) + 1e-8)\n",
    "    \n",
    "    # Compute CWT\n",
    "    coefficients, _ = pywt.cwt(signal, scales, wavelet)\n",
    "    \n",
    "    # Convert to image (normalize to 0-255)\n",
    "    cwt_abs = np.abs(coefficients)\n",
    "    cwt_normalized = (cwt_abs - cwt_abs.min()) / (cwt_abs.max() - cwt_abs.min() + 1e-8)\n",
    "    cwt_image = (cwt_normalized * 255).astype(np.uint8)\n",
    "    \n",
    "    return cwt_image\n",
    "\n",
    "print(\"Signal processing functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate CWT Images with Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(data_dict, segment_length=2048, overlap=0.5, augment=True):\n",
    "    \"\"\"\n",
    "    Generate CWT images from raw signals with augmentation\n",
    "    \"\"\"\n",
    "    X_images = []\n",
    "    y_labels = []\n",
    "    \n",
    "    print(\"\\nGenerating CWT images with augmentation...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for class_idx, (class_name, file_paths) in enumerate(data_dict.items()):\n",
    "        print(f\"\\nProcessing {class_name}...\")\n",
    "        class_images = 0\n",
    "        \n",
    "        for file_path in file_paths:\n",
    "            # Load signal\n",
    "            signal_data = load_signal_from_mat(file_path)\n",
    "            if signal_data is None:\n",
    "                continue\n",
    "            \n",
    "            # Segment signal\n",
    "            segments = segment_signal(signal_data, segment_length, overlap)\n",
    "            \n",
    "            for segment in segments:\n",
    "                # Apply augmentation\n",
    "                if augment:\n",
    "                    augmented_signals = augment_signal(segment, num_augmentations=2)\n",
    "                else:\n",
    "                    augmented_signals = [segment]\n",
    "                \n",
    "                for aug_signal in augmented_signals:\n",
    "                    # Generate CWT image\n",
    "                    cwt_img = signal_to_cwt_image(aug_signal, SCALES, WAVELET)\n",
    "                    \n",
    "                    # Resize to target size\n",
    "                    from scipy.ndimage import zoom\n",
    "                    zoom_factors = (IMAGE_SIZE[0] / cwt_img.shape[0], \n",
    "                                   IMAGE_SIZE[1] / cwt_img.shape[1])\n",
    "                    cwt_resized = zoom(cwt_img, zoom_factors, order=1)\n",
    "                    \n",
    "                    # Convert to RGB (3 channels)\n",
    "                    cwt_rgb = np.stack([cwt_resized] * 3, axis=-1)\n",
    "                    \n",
    "                    X_images.append(cwt_rgb)\n",
    "                    y_labels.append(class_idx)\n",
    "                    class_images += 1\n",
    "        \n",
    "        print(f\"  Generated {class_images} images\")\n",
    "    \n",
    "    X_images = np.array(X_images)\n",
    "    y_labels = np.array(y_labels)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Total images generated: {len(X_images)}\")\n",
    "    print(f\"Image shape: {X_images[0].shape}\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "    \n",
    "    return X_images, y_labels\n",
    "\n",
    "# Generate dataset\n",
    "X_data, y_data = generate_dataset(data_dict, SEGMENT_LENGTH, OVERLAP, augment=True)\n",
    "\n",
    "# Display class distribution\n",
    "unique, counts = np.unique(y_data, return_counts=True)\n",
    "print(\"\\nAugmented Dataset Distribution:\")\n",
    "for i, count in zip(unique, counts):\n",
    "    print(f\"  {CLASS_NAMES[i]:15s}: {count:4d} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Sample CWT Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize samples from each class\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i in range(4):\n",
    "    # Get random sample from each class\n",
    "    class_indices = np.where(y_data == i)[0]\n",
    "    sample_idx = np.random.choice(class_indices, 2, replace=False)\n",
    "    \n",
    "    for j, idx in enumerate(sample_idx):\n",
    "        ax = axes[i*2 + j]\n",
    "        ax.imshow(X_data[idx], cmap='jet', aspect='auto')\n",
    "        ax.set_title(f'{CLASS_NAMES[i]} - Sample {j+1}', fontweight='bold')\n",
    "        ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'sample_cwt_images.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Sample CWT images displayed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prepare Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset: 80% train, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_data, y_data, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y_data\n",
    ")\n",
    "\n",
    "# Normalize to [0, 1]\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to categorical\n",
    "y_train_cat = keras.utils.to_categorical(y_train, num_classes=4)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, num_classes=4)\n",
    "\n",
    "print(\"Dataset Split:\")\n",
    "print(f\"  Training samples:   {len(X_train)}\")\n",
    "print(f\"  Testing samples:    {len(X_test)}\")\n",
    "print(f\"\\nTraining set distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for i, count in zip(unique, counts):\n",
    "    print(f\"  {CLASS_NAMES[i]:15s}: {count:4d} samples\")\n",
    "print(f\"\\nTest set distribution:\")\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "for i, count in zip(unique, counts):\n",
    "    print(f\"  {CLASS_NAMES[i]:15s}: {count:4d} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Build EfficientNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape=(224, 224, 3), num_classes=4):\n",
    "    \"\"\"\n",
    "    Build EfficientNetB0 model for bearing fault classification\n",
    "    Fast training with high accuracy\n",
    "    \"\"\"\n",
    "    # Load pre-trained EfficientNetB0\n",
    "    base_model = EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers, fine-tune later layers\n",
    "    for layer in base_model.layers[:-30]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Build model\n",
    "    model = models.Sequential([\n",
    "        base_model,\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(256, activation='relu'),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Build model\n",
    "model = build_model()\n",
    "\n",
    "print(\"\\nModel Architecture:\")\n",
    "model.summary()\n",
    "\n",
    "print(f\"\\nTotal parameters: {model.count_params():,}\")\n",
    "print(f\"Trainable parameters: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Setup Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(\n",
    "        monitor='val_accuracy',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        os.path.join(OUTPUT_PATH, 'best_model.h5'),\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"Callbacks configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Starting Training...\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "history = model.fit(\n",
    "    X_train, y_train_cat,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Training Complete!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Accuracy plot\n",
    "ax1.plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)\n",
    "ax1.plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)\n",
    "ax1.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Model Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.legend(fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss plot\n",
    "ax2.plot(history.history['loss'], label='Train Loss', linewidth=2)\n",
    "ax2.plot(history.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "ax2.set_xlabel('Epoch', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Loss', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Model Loss', fontsize=14, fontweight='bold')\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'training_history.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print best metrics\n",
    "best_val_acc = max(history.history['val_accuracy'])\n",
    "best_epoch = history.history['val_accuracy'].index(best_val_acc) + 1\n",
    "\n",
    "print(f\"\\nBest Validation Accuracy: {best_val_acc*100:.2f}% (Epoch {best_epoch})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model = keras.models.load_model(os.path.join(OUTPUT_PATH, 'best_model.h5'))\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test_cat, verbose=0)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TEST SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Test Accuracy:  {test_accuracy*100:.2f}%\")\n",
    "print(f\"Test Loss:      {test_loss:.4f}\")\n",
    "print(\"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Generate Predictions and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_proba = model.predict(X_test, verbose=0)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES,\n",
    "            cbar_kws={'label': 'Count'},\n",
    "            annot_kws={'size': 14, 'weight': 'bold'})\n",
    "plt.xlabel('Predicted Label', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=14, fontweight='bold')\n",
    "plt.title(f'Confusion Matrix\\nTest Accuracy: {test_accuracy*100:.2f}%', \n",
    "          fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'confusion_matrix.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test, y_pred, target_names=CLASS_NAMES, digits=4))\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Per-Class Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-class metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "precision, recall, f1, support = precision_recall_fscore_support(\n",
    "    y_test, y_pred, average=None\n",
    ")\n",
    "\n",
    "# Create performance DataFrame\n",
    "performance_df = pd.DataFrame({\n",
    "    'Class': CLASS_NAMES,\n",
    "    'Precision': precision * 100,\n",
    "    'Recall': recall * 100,\n",
    "    'F1-Score': f1 * 100,\n",
    "    'Support': support\n",
    "})\n",
    "\n",
    "print(\"\\nPer-Class Performance:\")\n",
    "print(\"=\"*70)\n",
    "print(performance_df.to_string(index=False))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save to CSV\n",
    "performance_df.to_csv(os.path.join(OUTPUT_PATH, 'per_class_performance.csv'), index=False)\n",
    "\n",
    "# Visualize per-class performance\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(CLASS_NAMES))\n",
    "width = 0.25\n",
    "\n",
    "ax.bar(x - width, precision * 100, width, label='Precision', alpha=0.8)\n",
    "ax.bar(x, recall * 100, width, label='Recall', alpha=0.8)\n",
    "ax.bar(x + width, f1 * 100, width, label='F1-Score', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Class', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Score (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(CLASS_NAMES, fontweight='bold')\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim([95, 101])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'per_class_performance.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Save Model and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final model\n",
    "model.save(os.path.join(OUTPUT_PATH, 'hust_bearing_model_final.h5'))\n",
    "print(f\"✓ Model saved to: {os.path.join(OUTPUT_PATH, 'hust_bearing_model_final.h5')}\")\n",
    "\n",
    "# Save results summary\n",
    "results_summary = {\n",
    "    'Dataset': 'HUST Bearing',\n",
    "    'Total Original Samples': 99,\n",
    "    'Total Augmented Samples': len(X_data),\n",
    "    'Number of Classes': 4,\n",
    "    'Train Samples': len(X_train),\n",
    "    'Test Samples': len(X_test),\n",
    "    'Model': 'EfficientNetB0',\n",
    "    'Image Size': IMAGE_SIZE,\n",
    "    'Batch Size': BATCH_SIZE,\n",
    "    'Epochs Trained': len(history.history['accuracy']),\n",
    "    'Best Val Accuracy': f\"{best_val_acc*100:.2f}%\",\n",
    "    'Test Accuracy': f\"{test_accuracy*100:.2f}%\",\n",
    "    'Test Loss': f\"{test_loss:.4f}\"\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "import json\n",
    "with open(os.path.join(OUTPUT_PATH, 'results_summary.json'), 'w') as f:\n",
    "    json.dump(results_summary, f, indent=4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "for key, value in results_summary.items():\n",
    "    print(f\"{key:25s}: {value}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n✓ All results saved to: {OUTPUT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Sample Predictions Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some predictions\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "# Get random test samples\n",
    "random_indices = np.random.choice(len(X_test), 8, replace=False)\n",
    "\n",
    "for i, idx in enumerate(random_indices):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Display image\n",
    "    ax.imshow(X_test[idx], cmap='jet')\n",
    "    \n",
    "    # Get prediction\n",
    "    true_label = CLASS_NAMES[y_test[idx]]\n",
    "    pred_label = CLASS_NAMES[y_pred[idx]]\n",
    "    confidence = y_pred_proba[idx][y_pred[idx]] * 100\n",
    "    \n",
    "    # Set title with color based on correctness\n",
    "    color = 'green' if y_test[idx] == y_pred[idx] else 'red'\n",
    "    ax.set_title(f'True: {true_label}\\nPred: {pred_label} ({confidence:.1f}%)',\n",
    "                fontweight='bold', color=color, fontsize=10)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_PATH, 'sample_predictions.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Sample predictions visualized!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Final Summary for Research Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESEARCH PAPER SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nMETHODOLOGY:\")\n",
    "print(\"-\" * 70)\n",
    "print(\"1. Dataset: HUST Bearing Vibration Dataset\")\n",
    "print(f\"   - Original samples: 99 (Normal: 15, Ball: 12, Inner: 42, Outer: 30)\")\n",
    "print(f\"   - Augmented samples: {len(X_data)}\")\n",
    "print(f\"   - Sampling frequency: {SAMPLING_FREQUENCY} Hz\")\n",
    "print()\n",
    "print(\"2. Signal Processing:\")\n",
    "print(f\"   - Segmentation: {SEGMENT_LENGTH} samples with {OVERLAP*100}% overlap\")\n",
    "print(f\"   - CWT transform using {WAVELET} wavelet\")\n",
    "print(f\"   - Image size: {IMAGE_SIZE}\")\n",
    "print()\n",
    "print(\"3. Data Augmentation:\")\n",
    "print(\"   - Gaussian noise addition\")\n",
    "print(\"   - Amplitude scaling\")\n",
    "print(\"   - Time shifting\")\n",
    "print()\n",
    "print(\"4. Deep Learning Model:\")\n",
    "print(\"   - Architecture: EfficientNetB0 (transfer learning)\")\n",
    "print(f\"   - Total parameters: {model.count_params():,}\")\n",
    "print(f\"   - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"   - Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "print()\n",
    "print(\"=\"*70)\n",
    "print(\"RESULTS:\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Overall Test Accuracy:     {test_accuracy*100:.2f}%\")\n",
    "print(f\"Overall Test Loss:         {test_loss:.4f}\")\n",
    "print()\n",
    "print(\"Per-Class Performance:\")\n",
    "for i, class_name in enumerate(CLASS_NAMES):\n",
    "    print(f\"  {class_name:15s} - Precision: {precision[i]*100:.2f}%, \"\n",
    "          f\"Recall: {recall[i]*100:.2f}%, F1: {f1[i]*100:.2f}%\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n✓ All results and figures saved to:\", OUTPUT_PATH)\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
